---
title: KL divergence
sidebar:
  nav: docs-ko
aside:
  toc: true
key: 20201027
tags: 통계학 머신러닝
---

<p align = "center">
  <img src = "https://lh3.googleusercontent.com/proxy/-GwVQcne6_Lbqa30qyR_PGhEt0ap10_QIwcTT645HaeN7H5Bxr7vifeRoV3OLnLkpYUEPEs0bdLKggqhSLgBuzGATzT6yRxvmekiM7uz-oBkpedoZD9VO9BFLTthwcqbBxhk5NMG1m6_LMlTUjQfVZHgVJbUQBl4ywPQusVE570cFmqNnXo">
  <br>
  KL divergence가 말하는 것: 이상과 현실 간의 괴리
</p>

# prerequisites

해당 포스팅의 내용에 대해 이해하시려면 아래의 내용에 대해 알고 오시는 것을 추천드립니다.

* [정보 엔트로피](https://angeloyeo.github.io/2020/10/26/information_entropy.html)


