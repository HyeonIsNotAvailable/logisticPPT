---
title: Non-negative Matrix Factorization(NMF)
sidebar:
  nav: docs-ko
aside:
  toc: true
key: 20201015
tags: 선형대수 기계학습
---

$$X = WH$$

# W와 H의 의미

W: weights for the features

H: feature set

# 왜 non-negative feature를 뽑는게 좋을까?

# 알고리즘의 유도과정

목적 함수: euclidean distance from X to WH

$$D(X, WH) = \|X-WH\|^2_F =\sqrt{\sum_i^M\sum_j^N x_{ij}^2} = \sqrt{tr(X^TX)}$$

여기서 $tr(\cdot)$은 대각성분의 합을 말한다. 읽을 땐 trace라고 읽으면 된다.

다시 말해 trace 연산자는,

$$tr\left(\begin{bmatrix}a_{11} && a_{12} && a_{13} \\ a_{21} && a_{22} && a_{23}\\a_{31} && a_{32} && a_{33}\end{bmatrix}\right) = a_{11}+a_{22}+a_{33}$$

과 같다.

#

<p align = "center">
  <img src= "https://raw.githubusercontent.com/angeloyeo/angeloyeo.github.io/master/pics/2020-10-15-NMF/pic_face_dataset.png">
  <br>
</p>

<p align = "center">
  <img src= "https://raw.githubusercontent.com/angeloyeo/angeloyeo.github.io/master/pics/2020-10-15-NMF/pic_NMF_on_face.png">
  <br>
</p>


<p align = "center">
  <img src= "https://raw.githubusercontent.com/angeloyeo/angeloyeo.github.io/master/pics/2020-10-15-NMF/pic_PCA_on_face.png">
  <br>
</p>