---
title: Non-negative Matrix Factorization(NMF)
sidebar:
  nav: docs-ko
aside:
  toc: true
key: 20201015
tags: 선형대수 기계학습
---

# Prerequisites

이번 포스팅을 이해하기 위해선 아래의 내용에 대해 잘 알고 오시는 것을 추천드립니다.

* [주성분분석(PCA)](https://angeloyeo.github.io/2019/07/27/PCA.html)
* [특이값분해(SVD)](https://angeloyeo.github.io/2019/08/01/SVD.html)
* [독립성분분석(ICA)](https://angeloyeo.github.io/2020/07/14/ICA.html)
* [경사하강법(gradient descent)](https://angeloyeo.github.io/2020/08/16/gradient_descent.html)

독립성분분석은 내용이 어려운 편이기 때문에 꼭 다 이해하실 필요는 없습니다만, 주성분분석과 경사하강법은 알고오시는 것을 추천드립니다.

# NMF의 정의

$$X = WH$$

## W와 H의 의미

W: weights for the features

H: feature set

# 왜 NMF를 쓰는 것이 유용할 수 있을까?

## PCA, SVD의 한계점

PCA나 SVD는 feature들 간의 직교성이 보장됨.

PCA만 가지고 설명하면 PCA는 covariance matrix의 eigenvector를 이용한 분해인데, covariance matrix는 symmetric matrix이므로 eigenvector들은 항상 직교함.

이렇게 되면 데이터셋의 실제 데이터 구조를 잘 반영하지 못하게 될 수도 있음.

## geometrical interpretation of NMF

## 왜 non-negative feature를 뽑는게 좋을까?

# NMF의 update 규칙

$$H:= H\circ\frac{W^TX}{W^TWH}$$

$$W:= W\circ\frac{XH^T}{WHH^T}$$

여기서 '$\circ$'는 원소별 곱(Hadamard-product)을 표현한 것이다.

## update 알고리즘의 유도과정

목적 함수: euclidean distance from X to WH

$$D_{EUC}(X, WH) = \|X-WH\|^2_F =\sum_i^M\sum_j^N x_{ij}^2 = tr(X^TX)$$

여기서 $tr(\cdot)$은 대각성분의 합을 말한다. 읽을 땐 trace라고 읽으면 된다.

다시 말해 trace 연산자는,

$$tr\left(\begin{bmatrix}a_{11} && a_{12} && a_{13} \\ a_{21} && a_{22} && a_{23}\\a_{31} && a_{32} && a_{33}\end{bmatrix}\right) = a_{11}+a_{22}+a_{33}$$

과 같다.

업데이트의 방식은 gradient descent를 이용하자.

$$H:=H-\eta_H\circ \nabla_HD(X,WH)$$

$$W:=W-\eta_W\circ \nabla_WD(X,WH)$$


식 (X)를 조금 더 전개하면 아래와 같다.

$$\|X-WH\|_F^2 = tr\left((X-WH)^T(X-WH)\right)$$

$$=tr\left((X^T - H^TW^T)(X-WH)\right)$$

$$=tr\left(X^TX-X^TWH-H^TW^TX + H^TW^TWH\right)$$

여기서 trace 연산의 몇 가지 성질을 적어보자면 다음과 같다.

$$tr(A+B) = tr(A) + tr(B)$$

$$tr(ABC) = tr(CAB) = tr(BCA)$$

또, trace가 포함된 두 개 이상의 행렬곱에 대해 행렬에 대한 미분(즉, gradient)은 다음과 같다.

$$\nabla_X tr(AX) = A^T$$

$$\nabla_X tr(X^TA) = A$$

$$\nabla_X tr(X^TAX) = (A+A^T)X$$

$$\nabla_X tr(XAX^T) = X(A^T+A)$$

# NMF 적용 결과

<p align = "center">
  <img src= "https://raw.githubusercontent.com/angeloyeo/angeloyeo.github.io/master/pics/2020-10-15-NMF/pic_face_dataset.png">
  <br>
  데이터 셋의 일부
</p>

<p align = "center">
  <img src= "https://raw.githubusercontent.com/angeloyeo/angeloyeo.github.io/master/pics/2020-10-15-NMF/pic_NMF_on_face.png">
  <br>
  NMF 적용 결과
</p>


<p align = "center">
  <img src= "https://raw.githubusercontent.com/angeloyeo/angeloyeo.github.io/master/pics/2020-10-15-NMF/pic_PCA_on_face.png">
  <br>
  PCA 적용 결과
</p>

데이터셋이 가지고 있는 빛의 방향이나 형태적인 요소들을 NMF를 이용해 얻은 feature들이 잘 반영하고 있는 것을 알 수 있음.

또한, NMF의 feature들은 많은 값들이 0으로 채워져 있음. 즉, feature를 저장하는데 용량이 적게 들 수 있음.

또, PCA를 통해 얻은 feature들에 비해서 NMF로 얻은 feature들은 feature의 의미가 상대적으로 뚜렷해보임.

NMF를 이용하면 jpeg 처럼 데이터 통신에서도 유용하게 쓰일 수 있을 것 같아 보임. PCA는 압축하는 개념이라고 하면 NMF는 feature 자체가 몇개 되지도 않는데 weight만 주고 받음으로써 송수신이 가능할 것 같아 보임. (자세한 내용은 찾아볼 것)

## MATLAB 코드

```Matlab
clear; close all; clc;

load('YaleB_32x32.mat'); % gnd는 사람 번호인듯.
% 출처: http://www.cad.zju.edu.cn/home/dengcai/Data/FaceData.html
% 사용한 데이터셋의 이름은 Extended Yale Face Database B임.

figure('position',[556, 237, 947, 699]);
for i= 1:25
    subplot(5,5,i)
    imagesc(reshape(fea(i,:), 32, 32)); colormap('gray')
end

%% NMF 수행하기
n_features = 25;
[W, H] = nnmf(fea, n_features);

figure('position',[556, 237, 947, 699]);
for i_features = 1:n_features
    subplot(5,5,i_features)
    imagesc(reshape(H(i_features,:), 32, 32)); colormap('gray');
end

% figure; imagesc(reshape(randn(1, 25) * H, 32, 32)); colormap('gray')

%% PCA 수행하기

[coeff, score, latent] = pca(fea);

figure('position',[556, 237, 947, 699]);
for i_features = 1:n_features
    subplot(5,5,i_features)
    imagesc(reshape(coeff(:, i_features), 32, 32)); colormap('gray');
end



```

# 참고자료

* [위키피디아: 음수 미포함 행렬 분해](https://ko.wikipedia.org/wiki/%EC%9D%8C%EC%88%98_%EB%AF%B8%ED%8F%AC%ED%95%A8_%ED%96%89%EB%A0%AC_%EB%B6%84%ED%95%B4)
* [Detailed derivation of multiplicative update rules for NMF](https://www.jjburred.com/research/pdf/jjburred_nmf_updates.pdf)
* [NMF, k-means 를 이용한 토픽 모델링과 NMF, k-means + PyLDAvis 시각화](https://lovit.github.io/nlp/2019/06/10/visualize_topic_models_with_pyldavis/)
* [Document Clustering Based on Non-negative Matrix Factorization](https://people.eecs.berkeley.edu/~jfc/hcc/courseSP05/lecs/lec14/NMF03.pdf)
* [Introduction to deconICA](https://urszulaczerwinska.github.io/DeconICA/DeconICA_introduction.html)
* [NMF and Image Compression](https://www.kaggle.com/elenageminiani/nmf-and-image-compression)