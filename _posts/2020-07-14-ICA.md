---
title: 독립 성분 분석 (ICA)
sidebar:
  nav: docs-ko
aside:
  toc: true
key: 20200714
tags: 선형대수 통계학
---

# Prerequisites

아래의 내용에 대해 잘 알고 오는 것을 추천드립니다.

* [중심극한정리의 의미](https://youtu.be/iTNHQXGIEuU)
* Likelihood (우도)
  * 관련 내용 추천 Youtube 비디오: [StatQuest](https://www.youtube.com/watch?v=XepXtl9YKwc)
  * 위 영상 정리 blog: [BetterThanWholwas](https://jjangjjong.tistory.com/41)
* Gradient Descent(혹은 Gradient Ascent)

# ICA 모델과 목적

위키피디아에 따르면 독립 성분 분석(Independent Component Analysis, ICA)은 다변량의 신호를 통계적으로 독립적인 하부 성분으로 분리하는 계산 방법이라고 되어 있다.

말이 어려워 보이지만, 아래의 예시를 보면서 ICA가 할 수 있는 일이 어떤 것인지 알아보도록 하자.

<p align = "center">
  <img src = "http://pathpartner.wpengine.com/wp-content/uploads/2016/03/BSS-block-diagram.png">
  <br>
  그림 1. ICA가 할 수 있는 일을 block diagram으로 나타낸 것. 
  <br> 섞여버린 두 개의 음원을 분리해내는 과정을 보여주고 있다.
  <br>
  출처: https://www.pathpartnertech.com/blind-source-separation-for-cocktail-party-problem
</p>

방에서 두 사람이 동시에 말을 하고 있다고 생각해보자. 그리고 두 개의 마이크로 녹음을 진행하고 있다고 생각해보자.

두 개의 마이크로 녹음된 신호를 각각 $x_1(t), x_2(t)$라고 하고, 두 사람의 음성 신호를 각각 $s_1(t), s_2(t)$라고 하면,

이 식들의 관계를 다음과 같이 선형 방정식으로 표현할 수 있다.

$$x_1(t) = a_{11}s_1(t)+a_{12}s_2(t)$$

$$x_2(t) = a_{21}s_1(t)+a_{22}s_2(t)$$

여기서 $a_{ij}$ with $i,j = 1, 2$는 마이크로부터 발화자까지의 거리와 관련된 변수라고 하자.

우리가 원하는 것은 두 녹음 음원으로부터 원래의 음원을 분리해내는 일이다. 즉, $s_1(t)$와 $s_2(t)$를 복원해내는 일이다.


식 (1)과 식 (2)를 행렬로 표현하면 다음과 같다.

$$x = As$$

여기서 $A$를 mixing matrix라고 부르도록 하자.

또, 여기서 $x\in \mathbb{R}^n$, $A\in\mathbb{R}^{n\times n}$, $s\in\mathbb{R}^n$이다.

만약에 우리가 $A$를 알 수만 있다면 $A^{-1}$을 양변의 왼쪽에 곱해서 쉽게 source $s$를 찾을 수 있다.

즉, 아래와 같은 방법으로 source를 찾을 수 있는 것이다.

$$s = A^{-1}x = Wx$$

우리의 목적은 이 $W = A^{-1}$ 행렬을 찾는 것이다. 하지만, $A$ 행렬을 모르는 상태에서 어떻게 $W$ 행렬을 찾을 수 있을까? 


# 중심극한정리와 독립성분분석

독립 성분 분석(Independent Component Analysis, ICA)을 잘 이해하기 위해선 중심극한정리(Central Limit Theorem, CLT)에 대해 잘 알고 오는 것이 중요하다고 할 수 있다.

왜냐면 ICA의 개념은 CLT를 정 반대로 생각한 것이기 때문이다.

CLT를 짧게나마 복습해보면, 서로 독립적인 랜덤 변수들의 선형조합으로 이루어진 새로운 랜덤 변수의 분포는 가우스 분포를 따른다는 것이다.

특히 중요한 것은 선형 조합에 들어가는 독립 변수들이 많으면 많을 수록 더 가우스 분포에 가까워진다는 점이다.

CLT와 ICA를 각각 그림으로 표현하자면 아래와 같다.

// ***************************************************** //
// "그림 2 넣을 곳"
// CLT에 관련된 그림.
// 서로 독립적인 랜덤 변수들의 분포의 선형조합은 가우스 분포를 따른다.
// ***************************************************** //

// ***************************************************** //
// "그림 3 넣을 곳"
// ICA에 관련된 그림. 
// Gaussian 분포로 표현된 x들을 어떻게 조합하면 s가 나올까?
// ***************************************************** //

즉, ICA를 중심극한정리의 반대 과정으로 생각해본다면 ICA는 독립 랜덤 변수들의 조합으로 얻어진 $x$에 적절한 행렬 $W = A^{-1}$을 곱해

원래의 독립 랜덤 변수들인 source들인 $s$를 찾는 과정이다.


다시 말해 ICA에서는 **source들이 서로 독립적이라는 가정을 최대한 만족할 수 있도록하는 $W=A^{-1}$을 찾는 것** 을 목적으로 한다고 생각할 수 있다.

# Densities and linear transformation

독립성분분석에 대한 알고리즘을 이해하기 위해 필요한 마지막 개념으로써, 랜덤 변수에 선형변환을 적용했을 때 변환 적용 전 후 변수의 확률 밀도 간의 관계를 생각해보도록 하자.


$x=As$이고, $s=A^{-1}x=Wx$라는 점을 생각했을 때, 

$$p_x(x) = p_s(s)/|A|\notag$$

$$= p_s(s)\cdot|A^{-1}|=p_s(Wx)\cdot|W|$$

# ICA algorithm (Bell-Sejnowski algorithm)

각 source $s_i$의 density function을 $p_s$라고 하면, 모든 source로 구성된 joint distribution은 다음과 같다.

$$p(s) = \prod_{i=1}^{n}p_s(s_i)$$

바로 앞 꼭지에서 확인한 바와 같이 $x = As = W^{-1}s$라는 관계에서 $p(x)$를 구하면,

$$p(x) = \prod_{i=1}^{n}p_s(w_i^Tx)\cdot|W|$$

이다. 굳이 $p(x)$를 계산하는 것은 우리가 실제로 확인할 수 있는 데이터는 $s$가 아니라 $x$이기 때문이다.

---

//TODO: log likelihood를 계산하는 이유에 대해 서술

$$l(W) = \sum_{i=m}^{m}\left(\sum_{j=1}^{n}\log p_s(w_j^Tx^{(i)})+log|W|\right)$$

여기서 우리는 $p_s$에 대한 구체적인 확률밀도함수를 결정해야 한다.

일반적으로 $p_s$의 CDF로 $g(s) = 1/(1+e^{-s})$(즉, sigmoid 함수)를 사용했을 때 알고리즘의 사용에 문제가 없다고 알려져 있다. 

(만약 source의 density에 대한 사전지식이 있다면 해당 density function을 사용하면 된다.)

따라서, pdf인 $p_s$는 $g'(s)$이므로, 위 $l(W)$는 다음과 같이 쓸 수 있다.

$$l(W) = \sum_{i=m}^{m}\left(\sum_{j=1}^{n}\log g'(w_j^Tx^{(i)})+log|W|\right)$$

---

이제, $\nabla_W|W|=|W|(W^{-1})^T$ 라는 사실을 이용하여 $l(W)$를 $W$에 대해 편미분 해보자.

이 때, 수식 작성의 편의를 위해 $w_j^Tx^{(i)}$를 $s_j$로 치환해서 쓰도록 하자.

$$\frac{\partial l}{\partial W} = 

  \sum_{i=1}^{m}\left(\sum_{j=1}^{n}\frac{1}{g'(s_j)}\cdot g''(s_j)\cdot x^{(i)^T} + \frac{1}{|W|}|W|(W^{-1})^T\right)$$

$$= \sum_{i=1}^{m}\left(\sum_{j=1}^{n}\frac{1}{g(s_j)(1-g(s_j))}\cdot g(s_j)(1-g(s_j))(1-2g(s_j))\cdot x^{(i)^T} + (W^{-1})^T\right)$$

$$= \sum_{i=1}^{m}\left(\sum_{j=1}^{n}(1-2g(s_j))\cdot x^{(i)^T} + (W^{-1})^T\right)$$

Gradient Ascent 방법을 이용하면 $W$는 아래와 같이 update 할 수 있다.

$$W := W + \alpha \frac{\partial l}{\partial W}$$

따라서, 각 $j = 1, 2, \cdots, n$에 대해,

$$W := W + \alpha\left(\begin{bmatrix}
1 - 2g(w_1^Tx^{(i)}) \\
1 - 2g(w_2^Tx^{(i)}) \\
\vdots \\
1 - 2g(w_n^Tx^{(i)})
\end{bmatrix} x^{(i)^T} + (W^T)^{-1}\right)$$

여기서 $\alpha$는 learning rate이다.

위의 알고리즘이 수렴한 뒤에 얻은 $W$를 이용하면 원래의 sources, $s$를 얻을 수 있게 된다.

# ICA의 응용 사례

# 참고 문헌

1. Independent Component Analysis / CS229 Lecture Notes / Andrew Ng
2. Independent Component Analysis 1: Definition / Aapo Hyvarinen
3. Independent Component Analysis 2: Estimation by maximization of non-Gaussianity / Aapo Hyvarinen
4. Independent Component Analysis 3: Maximum Likelihood Estimation / Aapo Hyvarinen