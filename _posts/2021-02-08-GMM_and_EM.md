---
title: GMM과 EM 알고리즘
sidebar:
  nav: docs-ko
aside:
  toc: true
key: 20210208
tags: 통계학 머신러닝
---

# Prerequisites

이 포스트를 더 잘 이해하기 위해선 아래의 내용에 대해 알고 오시는 것을 추천드립니다.

* [k-means 알고리즘](https://angeloyeo.github.io/2021/02/07/k_means.html)
* [최대우도법(Maximum Likelihood Estimation)](https://angeloyeo.github.io/2020/07/17/MLE.html)

# GMM (Gaussiam Mixture Model)

[//]:# (우리가 하고자 하는 일은 바로 모수 추정이라는 사실을 계속 말해줄 것)

[//]:# (라벨이 주어진 데이터들을 plot 해주고 그것들의 mean과 var는 비교적 쉽게 구할 수 있음 <- MLE 문제)

[//]:# (하지만 만약 우리가 label을 알 수 없다면? --> GMM 문제)

## [복습] 최대우도법을 이용한 정규분포 fitting

이전 [최대우도법](https://angeloyeo.github.io/2020/07/17/MLE.html) 시간에서는 어떤 데이터를 관찰하고 그 데이터에 맞는 모수를 추정하는 방법을 알아보았다.

특히, 예시로써 확인해 본 것은 주어진 데이터에 대한 정규분포의 fitting 이었다.

우리에게 주어진 데이터를 $\lbrace x^{(1)}, x^{(2)}, \cdots, x^{(m)}\rbrace$이라 하자.

MLE 이용 시 데이터셋에 대해 정규분포를 가정한 뒤 평균과 분산을 추정하면 다음과 같은 평균, 분산 값을 얻게된다는 것 또한 확인하였다.

$$\hat{\mu}= \frac{1}{m}\sum_{i=1}^{m}x^{(i)}$$

$$\hat{\sigma}^2= \frac{1}{m}\sum_{i=1}^{m}(x^{(i)}-\mu)^2$$

위 결과는 우리가 익히 알고있는 평균, 분산에 관한 식이다. 하지만 이 결과는 주어진 데이터에 대한 우도(likelihood)를 계산한 뒤, 우도가 최대가 되게하는 평균과 분산 값을 얻은 결과라는 점을 기억해두도록 하자.

## EM 알고리즘을 이용한 GMM 모델링

Repeat until convergence: {

  $\quad$(E-step) For each $i$, $j$, set

  $$w_j^{(i)} := p(z^{(i)} = j|x^{(i)};\phi,\mu,\Sigma) $$

  $\quad$(M-step) Update the parameters:

  $$\phi_j := \frac{1}{m}\sum_{i=1}^{m}w_j^{(i)}$$

  $$\mu_j := \frac{\sum_{i=1}^{m}w_j^{(i)}x^{(i)}}{\sum_{i=1}^{m}w_j^{(i)}}$$

  $$\Sigma_j := \frac{\sum_{i=1}^{m}w_j^{(i)}\left(x^{(i)}-\mu_j\right)\left(x^{(i)}-\mu_j\right)^T}{\sum_{i=1}^{m}w_j^{(i)}}$$
}

베이즈 정리에 따르면,

$$p(z^{(i)}=j|x^{(i)};\phi,\mu,\Sigma) =\frac{p(x^{(i)}|z^{(i)} = j; \mu, \Sigma)p(z^{(i)}=j;\phi)}{p(x^{(i)};\phi, \mu, \Sigma)} $$

$$= \frac{p(x^{(i)}|z^{(i)} = j; \mu, \Sigma)p(z^{(i)}=j;\phi)}{\sum_{k=1}^{l}p(x^{(i)}|z^{(i)} = l; \mu, \Sigma)p(z^{(i)}=l;\phi)}$$

# EM (Expectation-Maximization) 알고리즘

## Jensen's Inequality

## EM 알고리즘의 수학적 유도


## GMM revisited

