---
title: GMM과 EM 알고리즘
sidebar:
  nav: docs-ko
aside:
  toc: true
key: 20210208
tags: 통계학 머신러닝
---

# Prerequisites

이 포스트를 더 잘 이해하기 위해선 아래의 내용에 대해 알고 오시는 것을 추천드립니다.

* [k-means 알고리즘](https://angeloyeo.github.io/2021/02/07/k_means.html)
* [최대우도법(Maximum Likelihood Estimation)](https://angeloyeo.github.io/2020/07/17/MLE.html)

# GMM (Gaussiam Mixture Model)

[//]:# (우리가 하고자 하는 일은 바로 모수 추정이라는 사실을 계속 말해줄 것)

[//]:# (라벨이 주어진 데이터들을 plot 해주고 그것들의 mean과 var는 비교적 쉽게 구할 수 있음 <- MLE 문제)

[//]:# (하지만 만약 우리가 label을 알 수 없다면? --> GMM 문제)

## [복습] 최대우도법을 이용한 정규분포 fitting

이전 [최대우도법](https://angeloyeo.github.io/2020/07/17/MLE.html) 시간에서는 어떤 데이터를 관찰하고 그 데이터에 맞는 모수를 추정하는 방법을 알아보았다.

특히, 예시로써 확인해 본 것은 주어진 데이터에 대한 정규분포의 fitting 이었다.

아래의 그림 1에서는 최대우도법을 이용하여 여러 평균값의 후보 중 적절한 것을 선택하는 예시를 보여주고 있다.

<p align = "center">
  <img width = "800" src = "https://raw.githubusercontent.com/angeloyeo/angeloyeo.github.io/master/pics/2020-07-17-MLE/pic1.png">
  <br>
  그림 1. 획득한 데이터와 추정되는 후보 분포 2개(각각 주황색, 파란색 곡선으로 표시)
</p>

좀 더 자세한 내용은 [최대우도법](https://angeloyeo.github.io/2020/07/17/MLE.html) 포스팅을 보는것이 좋겠으나 그림 1의 내용만을 간단히 얘기하자면, 수직선 상에 표시된 주황색 데이터들은 그림 1의 주황색 커브와 파란색 커브 중 주황색 커브로 부터 나왔을 가능성이 더 커보인다. 이것이 최대우도법의 핵심이었다.

그리고, 데이터에 대해 정규분포를 가정했을 때, 주어진 $m$개의 데이터에 대한 두 파라미터(평균, 분산)에 대한 최대우도법의 결론은 다음과 같았다.

우리에게 주어진 데이터를 $\lbrace x^{(1)}, x^{(2)}, \cdots, x^{(m)}\rbrace$이라 했을 때 평균, 분산 값을 계산할 때 likelihood 함수가 최대가 된다.

$$\hat{\mu}= \frac{1}{m}\sum_{i=1}^{m}x^{(i)}$$

$$\hat{\sigma}^2= \frac{1}{m}\sum_{i=1}^{m}(x^{(i)}-\mu)^2$$

위 결과는 우리가 익히 알고있는 평균, 분산에 관한 식이다. 하지만 이 결과는 주어진 데이터에 대한 우도(likelihood)를 계산한 뒤, 우도가 최대가 되게하는 평균과 분산 값을 얻은 결과라는 점을 기억해두도록 하자.


## 우리가 처한 문제는?

아래의 그림 2에서와 같이 수직선 $x$ 상에 


<p align = "center">
  <img src = "https://raw.githubusercontent.com/angeloyeo/angeloyeo.github.io/master/pics/2021-02-08-GMM_and_EM/pic2.png">
  <br>
  그림 2. Label이 주어진 Density Estimation 문제 (MLE를 이용해 풀 수 있음)
</p>


## EM 알고리즘을 이용한 GMM 모델링

Repeat until convergence: {

  $\quad$(E-step) For each $i$, $j$, set

  $$w_j^{(i)} := p(z^{(i)} = j|x^{(i)};\phi,\mu,\Sigma) $$

  $\quad$(M-step) Update the parameters:

  $$\phi_j := \frac{1}{m}\sum_{i=1}^{m}w_j^{(i)}$$

  $$\mu_j := \frac{\sum_{i=1}^{m}w_j^{(i)}x^{(i)}}{\sum_{i=1}^{m}w_j^{(i)}}$$

  $$\Sigma_j := \frac{\sum_{i=1}^{m}w_j^{(i)}\left(x^{(i)}-\mu_j\right)\left(x^{(i)}-\mu_j\right)^T}{\sum_{i=1}^{m}w_j^{(i)}}$$
  
}

베이즈 정리에 따르면,

$$p(z^{(i)}=j|x^{(i)};\phi,\mu,\Sigma) =\frac{p(x^{(i)}|z^{(i)} = j; \mu, \Sigma)p(z^{(i)}=j;\phi)}{p(x^{(i)};\phi, \mu, \Sigma)} $$

$$= \frac{p(x^{(i)}|z^{(i)} = j; \mu, \Sigma)p(z^{(i)}=j;\phi)}{\sum_{k=1}^{l}p(x^{(i)}|z^{(i)} = l; \mu, \Sigma)p(z^{(i)}=l;\phi)}$$

##

<p align = "center">
  <video width = "700" height = "auto" loop autoplay controls muted>
    <source src = "https://raw.githubusercontent.com/angeloyeo/angeloyeo.github.io/master/pics/2021-02-08-GMM_and_EM/pic1.mp4">
  </video>
  <br>
  그림 1. 
</p>

# EM (Expectation-Maximization) 알고리즘

## Jensen's Inequality

## EM 알고리즘의 수학적 유도


## GMM revisited

