---
title: GMM과 EM 알고리즘
sidebar:
  nav: docs-ko
aside:
  toc: true
key: 20210208
tags: 통계학 머신러닝
---

# Prerequisites

이 포스트를 더 잘 이해하기 위해선 아래의 내용에 대해 알고 오시는 것을 추천드립니다.

* [k-means 알고리즘](https://angeloyeo.github.io/2021/02/07/k_means.html)
* [최대우도법(Maximum Likelihood Estimation)](https://angeloyeo.github.io/2020/07/17/MLE.html)

# GMM (Gaussiam Mixture Model)

[//]:# (우리가 하고자 하는 일은 바로 모수 추정이라는 사실을 계속 말해줄 것)

[//]:# (라벨이 주어진 데이터들을 plot 해주고 그것들의 mean과 var는 비교적 쉽게 구할 수 있음 <- MLE 문제)

[//]:# (하지만 만약 우리가 label을 알 수 없다면? --> GMM 문제)

## [복습] 최대우도법을 이용한 정규분포 fitting

이전 [최대우도법](https://angeloyeo.github.io/2020/07/17/MLE.html) 시간에서는 어떤 데이터를 관찰하고 그 데이터에 맞는 모수를 추정하는 방법을 알아보았다.

특히, 예시로써 확인해 본 것은 주어진 데이터에 대한 정규분포의 fitting 이었다.

아래의 그림 1에서는 최대우도법을 이용하여 여러 평균값의 후보 중 적절한 것을 선택하는 예시를 보여주고 있다.

<p align = "center">
  <img width = "800" src = "https://raw.githubusercontent.com/angeloyeo/angeloyeo.github.io/master/pics/2020-07-17-MLE/pic1.png">
  <br>
  그림 1. 획득한 데이터와 추정되는 후보 분포 2개(각각 주황색, 파란색 곡선으로 표시)
</p>

좀 더 자세한 내용은 [최대우도법](https://angeloyeo.github.io/2020/07/17/MLE.html) 포스팅을 보는것이 좋겠으나 그림 1의 내용만을 간단히 얘기하자면, 수직선 상에 표시된 주황색 데이터들은 그림 1의 주황색 커브와 파란색 커브 중 주황색 커브로 부터 나왔을 가능성이 더 커보인다. 이것이 최대우도법의 핵심이었다.

그리고, 데이터에 대해 정규분포를 가정했을 때, 주어진 $m$개의 데이터에 대한 두 파라미터(평균, 분산)에 대한 최대우도법의 결론은 다음과 같았다.

우리에게 주어진 데이터를 $\lbrace x^{(1)}, x^{(2)}, \cdots, x^{(m)}\rbrace$이라 했을 때 평균, 분산 값을 계산할 때 likelihood 함수가 최대가 된다.

$$\hat{\mu}= \frac{1}{m}\sum_{i=1}^{m}x^{(i)}$$

$$\hat{\sigma}^2= \frac{1}{m}\sum_{i=1}^{m}(x^{(i)}-\mu)^2$$

위 결과는 우리가 익히 알고있는 평균, 분산에 관한 식이다. 하지만 이 결과는 주어진 데이터에 대한 우도(likelihood)를 계산한 뒤, 우도가 최대가 되게하는 평균과 분산 값을 얻은 결과라는 점을 기억해두도록 하자.


## 우리가 처한 문제는?

아래의 그림 2에서와 같이 수직선 상에 데이터가 10개 주어져 있다고 생각해보자.

여기서 우리에게는 각 데이터에 label이 주어져있다고 해보자. (동그라미 안의 색깔이 서로 다른 label을 의미한다.)

<p align = "center">
  <img src = "https://raw.githubusercontent.com/angeloyeo/angeloyeo.github.io/master/pics/2021-02-08-GMM_and_EM/pic2.png">
  <br>
  그림 2. Label이 주어진 Density Estimation 문제 (MLE를 이용해 풀 수 있음)
</p>

그러면, 이 데이터 셋에 대한 density estimation을 할 수 있을까? 

여러가지 방법으로 density estimation을 할 수 있지만, 모수적인 방법으로 density estimation을 한다고 하면 우선 분포를 가정하고 거기에 맞는 모수들을 추정해야 한다. 앞서 언급한 방법인 최대우도법(MLE)은 주어진 데이터에 대해 분포의 모수를 추정할 수 있게 해주는 아주 좋은 방법이라고 할 수 있다. 아래의 그림 3은 그림 2에서 주어진 데이터들에 대해 정규분포를 가정하고 모수를 추정해 density estimation을 수행한 결과라고 할 수 있다. 모수 추정 방법은 앞서 언급한 MLE의 결과를 이용하여 각 label에 해당하는 데이터들의 평균값과 표준편차 값을 계산함으로써 진행될 수 있다.

<p align = "center">
  <img src = "https://raw.githubusercontent.com/angeloyeo/angeloyeo.github.io/master/pics/2021-02-08-GMM_and_EM/pic3.png">
  <br>
  그림 3. label이 주어진 데이터와 이에 대해 추정한 두 분포
</p>

그런데, 만약 우리에게 label이 주어지지 않는다면 어떨까?

<p align = "center">
  <img src = "https://raw.githubusercontent.com/angeloyeo/angeloyeo.github.io/master/pics/2021-02-08-GMM_and_EM/pic4.png">
  <br>
  그림 4. label이 주어지지 않은 데이터들의 분포
</p>

위의 그림 4에서는 그림 2와 같은 데이터에 라벨이 주어지지 않은 경우를 보여주고 있다.

그림 4에서와 같이 라벨이 주어져 있지 않은 데이터에 라벨을 부여하기 위한 가장 합리적인 방법은 무엇일까?

바로 likelihood의 비교이다. 

즉, 두 개의 분포가 주어졌다고 하면, 각 데이터에 대한 분포의 높이값을 비교하여 라벨링을 해줄 수 있다.

이 때, 이 두 분포는 처음에 정말 우연하게 이렇게 주어졌으며, 주황색과 파란색을 각각 그룹 1, 그룹 2라고 했을 때, 각 그룹의 평균과 표준편차는 다음과 같았다고 해보자.

$$\mu_1 = 3, \sigma_1 = 2.9155$$

$$\mu_2 = 10, \sigma_2 = 3.9623$$

<p align = "center">
  <img src = "https://raw.githubusercontent.com/angeloyeo/angeloyeo.github.io/master/pics/2021-02-08-GMM_and_EM/pic5.png">
  <br>
  그림 5. 두 label(0, 1)에 대한 분포가 만약에 주어진다면?
</p>

위의 그림 5와 같은 분포가 random 하게 주어졌다고 하면 그 분포에 맞추어서 각각의 데이터들의 label을 선정할 수 있다. 아래의 그림 6에서는 $x=9$인 데이터 샘플의 label을 선정하고자 하는 과정을 보여주고 있다.

<p align = "center">
  <img src = "https://raw.githubusercontent.com/angeloyeo/angeloyeo.github.io/master/pics/2021-02-08-GMM_and_EM/pic6.png">
  <br>
  그림 6. 주어진 분포를 이용해 각각의 데이터 샘플들의 라벨을 선정할 수 있다. 이 경우 $x = 9$에 해당하는 데이터의 label은 1이 될 것이다.
</p>

그림 6의 방법을 이용하여 모든 데이터 샘플들에 대한 label을 확인하면 다음의 그림7과 같은 결과를 얻을 수 있을 것이다.

<p align = "center">
  <img src = "https://raw.githubusercontent.com/angeloyeo/angeloyeo.github.io/master/pics/2021-02-08-GMM_and_EM/pic7.png">
  <br>
  그림 7. 
</p>

자 그럼 이번에는 그림 7에서 확인한 label에 따라 각 그룹의 분포를 예측해보자.

각 그룹의 평균, 표준편차는 다음과 같다.

$$\mu_1 = 4, \sigma_1 = 2.1602$$

$$\mu_2 = 21.33, \sigma_2 = 7.0048$$

이 값을 이용해 


## EM 알고리즘을 이용한 GMM 모델링

Repeat until convergence: {

  $\quad$(E-step) For each $i$, $j$, set

  $$w_j^{(i)} := p(z^{(i)} = j|x^{(i)};\phi,\mu,\Sigma) $$

  $\quad$(M-step) Update the parameters:

  $$\phi_j := \frac{1}{m}\sum_{i=1}^{m}w_j^{(i)}$$

  $$\mu_j := \frac{\sum_{i=1}^{m}w_j^{(i)}x^{(i)}}{\sum_{i=1}^{m}w_j^{(i)}}$$

  $$\Sigma_j := \frac{\sum_{i=1}^{m}w_j^{(i)}\left(x^{(i)}-\mu_j\right)\left(x^{(i)}-\mu_j\right)^T}{\sum_{i=1}^{m}w_j^{(i)}}$$
  
}

베이즈 정리에 따르면,

$$p(z^{(i)}=j|x^{(i)};\phi,\mu,\Sigma) =\frac{p(x^{(i)}|z^{(i)} = j; \mu, \Sigma)p(z^{(i)}=j;\phi)}{p(x^{(i)};\phi, \mu, \Sigma)} $$

$$= \frac{p(x^{(i)}|z^{(i)} = j; \mu, \Sigma)p(z^{(i)}=j;\phi)}{\sum_{k=1}^{l}p(x^{(i)}|z^{(i)} = l; \mu, \Sigma)p(z^{(i)}=l;\phi)}$$

##

<p align = "center">
  <video width = "700" height = "auto" loop autoplay controls muted>
    <source src = "https://raw.githubusercontent.com/angeloyeo/angeloyeo.github.io/master/pics/2021-02-08-GMM_and_EM/pic1.mp4">
  </video>
  <br>
  그림 1. 
</p>

# EM (Expectation-Maximization) 알고리즘

## Jensen's Inequality

## EM 알고리즘의 수학적 유도


## GMM revisited

