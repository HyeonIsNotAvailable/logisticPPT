---
title: Restricted Boltzmann Machine
sidebar:
  nav: docs-ko
aside:
  toc: true
key: 20201002
tags: 기계학습
---

# 확률분포를 알 수만 있다면...

Restricted Boltzmann Machine(이하 RBM)은 Generative Model이라고 하는데, ANN, DNN, CNN, RNN 등과 같은 Deterministic Model들과 약간 다른 목표를 갖고 있다.

Deterministic Model들이 타겟과 가설 간의 차이를 줄여서 오차를 줄이는 것이 목표라고 한다면, Generative Model들의 목표는 확률밀도함수를 모델링하는 것이다.

확률 밀도 함수(probability density function, pdf)를 정확히 안다는 것은 무엇일까?

가령 얼굴을 그려주는 기계가 있다고 하자. 얼굴은 여러가지 요소로 구성되어 있는데, 가령 코를 그려준다고 해보자.

코 역시도 여러가지 다양한 가능한 경우의 형태로 구성되어 있다. 여기서 만약, 세상에 코의 형태가 동그라미, 세모, 네모 코만 있다고 가정했을 때, 

우리가 이 세가지 형태의 코에 대한 확률밀도함수를 알 수 있다고 하자. 즉, 온 세상의 얼굴에서 코의 형태를 다 조사해 histogram을 그려보았다고 해보자.

<p align = "center">
  <img width = "400" src = "https://raw.githubusercontent.com/angeloyeo/angeloyeo.github.io/master/pics/2020-10-02-RBM/pic1.png">
  <br>
  그림 1. 코의 모양에 대한 확률 분포
</p>

위의 그림을 보면 세모(▲) 모양의 코가 전 세상에서 가장 흔하다는 것을 알 수 있다.

아마 얼굴 그려주는 기계가 이런 확률 분포에 대해 알고 있다면 얼굴을 그릴 때 코에 대해서는 세모 모양의 코를 그려줄 가능성이 좀 더 높아 보인다.

이런 식으로 모든 가능한 경우에 대해 어떤 사건이 발생할 확률을 정확히 알 수 있다면, 여러 사건들의 조합으로 구성되는 사건(즉, 여기서는 전체 얼굴)을 일리있게 **생성**할 수 있게 되는 것이다.

실제로 Generative Model 중 요즘 유행하는 GAN(Generative Adversarial Networks)을 이용해 생성된 얼굴은 다음 그림에서 볼 수 있다.

<p align = "center">
  <img width = "600" src = "https://www.researchgate.net/publication/341699736/figure/fig2/AS:896007655149568@1590636280012/45-years-of-GAN-progress-on-face-generation-20147-201510-201611-201712.png">
  <br>
  그림 2. GAN을 이용해 만들어진 얼굴 변천사
  <br>
  <a href = "https://www.researchgate.net/publication/341699736_Generative_Adversarial_Networks_GANs_An_Overview_of_Theoretical_Model_Evaluation_Metrics_and_Recent_Developments"> 그림 출처 </a>
</p>

이렇듯 확률밀도함수를 통해 결과물(여기서는 얼굴)을 **생성**해주는 과정을 샘플링(sampling)이라고 한다.

즉, Generative Model의 목적은 확률분포를 정확히 학습해 좋은 sample을 sampling하는 것이라고 정리할 수 있을 것이다.

## 확률밀도함수를 학습하기 위한 머신 설계

### Boltzmann Machine

Boltzmann Machine은 이렇듯 확률분포(정확히는 확률질량함수 혹은 확률밀도함수)를 학습하기 위해 만들어졌다고 할 수 있다.

Boltzmann Machine이 가정하는 것은 "우리가 보고 있는 것들 외에도 보이지 않는 요소들까지 잘 포함시켜 학습할 수 있다면 확률분포를 좀 더 정확하게 알 수 있지 않을까?"라는 것이다.

<p align = "center">
  <img src = "https://raw.githubusercontent.com/angeloyeo/angeloyeo.github.io/master/pics/2020-10-02-RBM/pic3.png">
  <br>
  그림 3. Boltzmann Machine과 Restricted Boltzmann Machine
  <br>
  <a href = "https://www.asimovinstitute.org/neural-network-zoo/"> 그림 출처 </a>
</p>

그림 3의 왼쪽에 보이는 것이 Boltzmann Machine이고 오른쪽에 보이는 것이 Restricted Boltzmann Machine이다.

우선 Boltzmann Machine에 대해서부터 설명하자면 동그랗게 생긴 것들이 가능한 이벤트들에 대한 state이다.

가령, 이 Boltzmann Machine이 얼굴의 형태에 관한 확률밀도함수를 학습하기 위해 만들어졌고, 각 동그라미들은 얼굴의 특정 부위에 대한 상태를 표시한다고 하자.

그 중 하나의 동그라미가 코에 관한 것이고, 상태 0, 1, 2가 동그라미, 세모, 네모 모양의 코에 대한 state를 각각 나타내는 것이라고 할 수 있겠다.

Boltzmann Machine에서 또 하나 주목할 점은 노란색과 초록색으로 표현된 특성인데, 각각은 hidden unit과 visible unit의 존재를 표현한 것이다.

hidden unit이 말하는 것은 우리가 보지 못하는 어떤 특성이 존재함을 암시하고, 이것 까지도 학습할 수 있다면 좀 더 정확한 확률분포를 학습할 수 있다는 것을 전제한다.

### Restricted Boltzmann Machine

그렇다면 우리의 관심사인 Restricted Boltzmann Machine(RBM)은 뭘까?

RBM은 그림 3에서 오른쪽에 표시되어 있는데, RBM은 Boltzmann Machine에서부터 파생되어 나온 것으로 visible unit과 hidden unit에는 내부적인 연결이 없고, visible unit과 hidden unit 간의 연결만이 남아있는 형태이다.

이렇게 RBM을 구성한 것은 상당히 실용적인 이유로 사건 간의 독립성을 가정하면 확률분포의 결합을 표현하기 쉬워지기 때문이다.

쉽게 말하면 Boltzmann Machine의 계산이 너무 복잡해지니까 이를 편하게 하기 위해 덜 엄격한 모델을 구성한 것으로 볼 수 있다.

Boltzmann Machine에서 RBM과 같은 형태를 구성하게 됨으로써 생기는 독특한 점은 RBM은 **Feed-Forward Nerual Network(FFNN) 처럼 학습**하게 된다는 점이다.

뒤에 더 설명하겠지만, RBM의 작동방식은 FFNN과 유사하게 forward propagation을 통해 hidden unit의 상태를 결정하고, 다시 hidden unit의 상태로부터 back propagation을 함으로써 visible unit의 상태를 재결정하게 된다.

그렇지만 RBM을 다루면서도 잊지 않아야 할 것은 결국 RBM도 Boltzmann Machine에서부터 파생된 것으로 현상에 대한 확률분포를 학습하는 Generative Model이라는 점이다.


# RBM의 학습 과정

위에서 서술했듯이 RBM이 무언가를 학습한다는 것은 확률분포를 습득하는 것이다.

그렇다면 RBM이 학습을 잘 했다는 것을 어떻게 수학적으로 표현할 수 있을까?

그것을 알기 위해 우선은 RBM의 구조에 대해서 조금 더 자세하게 알아보도록 하자.

## RBM의 구조

<p align = "center">
  <img src = "https://i.imgur.com/sadnLks.png">
  <br>
  그림 4. RBM의 조금 더 구체적인 구조
  <br> <a href = "https://i.imgur.com/sadnLks.png"> 그림 출처 </a>
</p>

TODO: $v$와 $b$ 그리고 $h$와 $c$에 대해 설명하고, $W$의 의미에 대해 설명할 것.

visible layer와 hidden layer의 bias가 갖는 의미: 각 visible node와 hidden node의 inherent property를 찾아주는 역할

## Energy: Configuration을 반영하는 수치

RBM이 학습을 잘 했다는 것을 수치적으로 판단하기 위해서는 여타 model들을 구성할 때와 마찬가지로 cost function을 정의해주어야 한다.

RBM의 cost function은 독특하게 "Energy"라는 개념으로 설명하는데, 이 energy라는 개념은 물리학에서 차용한 것이라고 한다.

Energy는 그 값이 낮을 수록 안정적인 상태를 의미하며, 이렇듯 안정적인 상태에 돌입했다는 것은 RBM이 학습을 잘 했음을 의미한다.

다시 말해, energy가 낮다는 것은 우리가 원하는 특성에 맞는 확률분포를 잘 학습했음을 의미하기도 한다.

특정 상태 $x$에 대한 에너지를 $E(x)$라고 한다면, 그에 맞는 확률 분포는 다음과 같이 쓸 수 있다.

$$p(x) = \frac{\exp(-E(x))}{Z}$$

$$\text{where }Z=\sum_x\exp(-E(x))$$

이 확률분포 $p(x)$의 의미는 다항분포로부터 얻었다고 할 수 있는데, 가능한 모든 경우의 $x$에 대한 energy 함수 중, 우리가 원하는 $x$에 대한 에너지를 얻을 확률을 계산해둔 것이라고 할 수 있다.

---

우리는 visible units $v$와 hidden units $h$의 상태에 따라 에너지를 결정할 수 있게 되는데, 결국 우리가 관찰할 수 있는 것은 visible units $v$에 의한 energy 이므로 다음과 같이 visible units의 확률 분포를 결정할 수 있다.

$$p(v) = \sum_h p(v, h) = \sum_h \frac{\exp(-E(v, h))}{Z}$$

$$\text{where }Z = \sum_v \sum_h \exp(-E(v,h))$$


TODO: 이러한 확률 분포를 계산하는 것은 사실상 불가능함을 설명할 것.

Free energy의 개념 도입: $Z$를 계산하는 것은 불가능하고 . 또, exponential term이 너무 많으니까 계산이 어려우므로 log를 붙여줘서 계산을 편하게 할 수 있음.

또 state를 매우 단순화 시키는 과정 도입: visible unit과 hidden unit은 0 또는 1의 상태만 갖는다고 가정함으로써 계산을 쉽게 할 수 있게 됨.

## RBM에게 학습이란: Contrastive Divergence를 줄이는 것

Contrastive Divergence(CD)란 visible layer에 데이터를 주고 hidden layer의 node 값을 샘플링, 그런 다음 다시 visible layer의 데이터를 다시 예측하도록 하는 과정에서 처음 주어진 visible layer의 데이터와 다시 획득한 visible layer의 데이터가 얼마나 차이가 있는지를 말하는 것.

만약 weight matrix와 hidden layer의 bias가 잘 학습되었다면 hidden unit을 잘 sample 해줄 수 있을 것이고 그렇다면 재획득한 visible layer의 데이터는 원래의 데이터와 일치할 것이기 때문이다.

즉, 원래의 데이터로부터 얻은 visible layer의 free energy와 생성된 데이터로부터 얻은 visible layer의 free energy 간의 차이가 적을 수록 학습이 잘 된 것이라고 할 수 있으므로 loss는 다음과 같이 생각할 수 있다.

$$loss = F(v)-F(v^{(1)})$$

### Sampling 과정

TODO: Gibbs sampling에 대해 설명.

TODO2: Gibbs sampling의 과정을 조금 더 구체적으로 설명. 실제로는 주어진 visible unit 혹은 hidden unit을 놓고 Rejection Sampling이 일어남을 설명

# RBM 의 학습 코드 (Python)

RBM의 학습 코드는 Python을 기반으로 했으며 Pytorch를 이용해 최적화하였다.

[Pytorch](https://pytorch.org/)로 포팅한 코드의 출처는 [이곳](https://github.com/odie2630463/Restricted-Boltzmann-Machines-in-pytorch)이며, 원래의 [Theano](http://www.deeplearning.net/software/theano/)를 이용해 포팅했던 소스코드는 [이곳](http://deeplearning.net/tutorial/rbm.html)이 참고 되었음을 밝힌다.

TODO: 글을 다 쓰고 가장 마지막에 학습 코드 붙일 것. HTML 코드 너무 길어... 최대한 주석을 달아줄 것.

# RBM 학습 결과

