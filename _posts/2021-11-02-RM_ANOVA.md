---
title: 반복측정 분산분석(Repeated Measures ANOVA)
sidebar:
  nav: docs-ko
aside:
  toc: true
key: 20211102
tags: 통계학
---

이 포스팅은 Primer of Biostatistics, S. Glantz, 7판을 참고하여 작성하였습니다.

# Prerequisites

이 포스팅의 내용을 잘 이해하기 위해선 아래의 내용에 대해 알고 오시는 것을 추천합니다.

* [표본과 표준 오차의 의미](https://angeloyeo.github.io/2020/02/12/standard_error.html)
* [F-value의 의미와 분산분석](https://angeloyeo.github.io/2020/02/29/ANOVA.html)
* [대응 표본 t-검정](https://angeloyeo.github.io/2021/10/29/paired_t_test.html)

# Motivation

대응 표본 t-검정에서는 피험자들의 before / after 차이를 확인할 수 있었다. 그런데, 어떤 경우에는 애프터 이후의... 삼프터, 사프터(??)도 한번에 비교하고 싶을 때도 있지 않을까?

좋은 예시 중 하나로 의학적 처치 후의 팔로우-업(follow-up)이 있다.

팔로우-업은 처치의 before / after를 확인 후 일정 기간이 지난 뒤에 한번 더(가령 1년 후) 검진을 수행해 경과를 관찰하는 것을 말한다.

즉, 팔로우 업을 한번만 수행한다면 동일 피험자에 대해 세 번의 시간에 걸쳐 측정이 진행되는 것과 마찬가지다[^1].

[^1]: 추후에 더 서술하겠지만 반복 측정이 꼭 시간에 한정되는 개념은 아니다. 시간적이거나 공간적인 차원에서 모두 반복 측정이 가능하다.

이것을 분석하기 위한 통계적 기법이 바로 반복측정 분산분석(Repeated Measures ANOVA)이며 줄여서 RM ANOVA라고도 많이 부른다.

# 제곱합(Sum of Squares; SS)

분산분석을 공부할 때 있어서 제곱합이라는 개념이 가장 큰 걸림돌이 된다. 처음 들으면 다소 생소한 개념일 수 있으나 제곱합의 개념은 분산 분석에서 아주 중요한 개념이다. 일단은 제곱합을 왜 사용해야 할까?

보통 분산 분석에서 제곱합이라고 하면 편차 제곱합(sum of squares of difference)를 많이 얘기한다.

그러면 우리가 생각해봐야 하는 것은 두 가지이다. 왜 편차에 관심을 가져야 하고 제곱합에 관심을 가져야 할까?

우선, 편차에 대해 생각해보자. 어떤 비교든지 간에 비교의 시작은 빼기(-)를 수행해줘야 비교할 수 있다. 그렇게 어려울 것이 없다. 비교를 위해서 편차를 생각하는 것은 자연스러운 논리적 흐름이라고 할 수 있다.

그럼 제곱은 왜 해줄까? 우선은 부호를 제거해주기 위한 목적이 있다. 편차는 양수, 음수 모두 나올 수 있기 때문에 합해주는 과정에서 복잡함이 생긴다. 절대값을 씌워줄 수도 있지만 그것보다는 제곱을 취하는 편이 계산에 편리하다. 따라서, 부호에 관계 없이 '변동'의 의미만을 남기고자 하는 것이다. 

그런데, 제곱합을 이용하는 것이 끝까지 살아남은 이유는 전체 제곱합은 특별한 의미를 지닌 제곱합들로 쪼개 생각할 수 있기 때문이다. 무슨 말인지 감이 오지 않을텐데, 뒤에서 더 설명할 "ANOVA를 SS 관점에서 이해해보기"를 들여다보면 더 깊게 이해할 수 있을 것이다. 

이 시점부터는 제곱합을 SS(Sum of Squares)라고 줄여 적도록 하겠다.

## 용어 정리

SS를 이용해 ANOVA를 이해해보기에 앞서 용어를 간단하게 정리하고 넘어가도록 하자.

* $SS_{something}$이라고 쓰면 something에 의해서 설명되는 제곱합이다. 

* 자유도(degree of freedom; DF)는 주어진 조건 안에서 통계적인 추정을 할 때에 표본이 되는 자료 중에 모집단에 대한 정보를 주는 독립적인 자료의 수를 말한다.

$DF_{something}$이라고 쓰면 something이라는 조건에 관한 자유도를 말한다.

* 평균 제곱(mean square; MS)은 SS의 평균으로써, 산술적 평균이 아니라 SS를 자유도로 나눈 값이다. 

즉, 평균적인 편차라는 의미에서 일종의 분산 역할을 한다. 다만 분산과 개념을 구분시켜 생각하는 이유는 MS는 여러가지 이유로 자유도가 수정되면 수정될 수 있는 통계치이기 때문이다.

## ANOVA를 SS의 관점에서 이해해보기

우리는 [F-value의 의미와 분산분석](https://angeloyeo.github.io/2020/02/29/ANOVA.html) 편에서 분산분석을 수행하는 과정을 확인해보았다.

분산분석은 기본적으로 모든 샘플 집단이 하나의 모집단에서 나왔다는 귀무가설을 가지고 진행된다.

그래서 각 샘플 집단 내의 분산과 각 샘플 집단 평균들 간의 분산을 계산하고 그 비율을 계산했다. 

만약 집단 내의 분산에 비해 샘플 집단 평균 간의 분산이 너무 크다면 우리는 귀무가설이 맞기 어려울 것이라고 보고 귀무가설을 기각해 적어도 하나의 샘플 집단은 다른 모집단에서 추출되었을 것이라고 볼 수 있다고 했다.

이 때, 분산의 비율값을 F 값이라고 불렀다. 즉, F 값을 수식으로 쓰면,

$$F=\frac{s^2_{bet}}{s^2_{wit}}$$

이다. 여기서 $s^2_{bet}$는 그룹 간 분산의 추정값, $s^2_{wit}$는 그룹 내 분산의 추정값을 말한다.

이 F 값은 자주 사용되는 통계치로써 분자, 분모에 들어가는 분산의 자유도에 맞춘 F 값의 분포가 알려져 있다. 

그러므로 이에 따라 주어진 샘플 그룹으로부터 계산한 F 값이 얼마나 큰 값인지 계산할 수 있는 것이고 이를 통해 통계적 유의성을 검증한다.

(만약 위 내용이 잘 이해되지 않는다면 [F-value의 의미와 분산분석](https://angeloyeo.github.io/2020/02/29/ANOVA.html) 편을 보시는 것을 추천드립니다.)

---

이제 앞서 설명한 F 값을 평균제곱(MS)로 쓰면 다음과 같이 쓸 수도 있다.

$$F =\frac{s^2_{bet}}{s^2_{wit}} = \frac{MS_{bet}}{MS_{wit}}$$

용어 정리 부분에서 설명한 바와 같이 MS는 분산의 개념과 거의 같은 것으로 볼 수 있기 때문에 이렇게 쓰더라도 문제가 없다.

여기서 MS를 SS와 자유도로 한번 더 나눠 쓰면 아래와 같다.

$$F = \frac{MS_{bet}}{MS_{wit}} = \frac{SS_{bet}/DF_{bet}}{SS_{wit}/DF_{wit}}$$




# RM ANOVA

## RM ANOVA에 사용되는 데이터 구조

우선은 

<p align = "center">
  <img width = "600" src = "https://raw.githubusercontent.com/angeloyeo/angeloyeo.github.io/master/pics/2021-11-02-RM_ANOVA/pic1.png">
  <br>
  그림 1. 반복측정 데이터를 표와 기호로 정리한 것
</p>

## Sum of Squares 의 분할(증명)

$$SS_{tot} = \sum_{t}\sum_{s}(X_{ts}-\bar{X})^2$$

Let 

$$a_{ts} = X_{ts} - \bar{S_s}$$

where 

$$\bar{S_s} = \frac{1}{m}\sum_{t}X_{ts}$$

and

$$b_s = \bar{S_s}-\bar{X}$$

then,

$$X_{ts}-\bar{X} = X_{ts}-\bar{S_s} + \bar{S_s} - \bar{X}$$

$$=a_{ts}+b_s$$

We can figure out that...

$$\sum_{t}a_{ts}=\sum_t\left(X_{ts}-\bar{S_s}\right)$$

$$=\left(\sum_{t}X_{ts}\right)-m\bar{S_s}$$

$$=\sum_{t}X_{ts}-m\cdot\frac{1}{m}\sum_{t}X_{ts} = 0$$

Hence,

$$SS_{tot}=\sum_{t}\sum_{s}(X_{ts}-\bar{X})^2$$

$$=\sum_{t}\sum_{s}\left(a_{ts}+b_s\right)^2$$

$$=\sum_{t}\sum_{s}\left(a_{ts}^2+b_s^2+2a_{ts}b_s\right)$$

$$=\sum_{t}\sum_{s}\left(a_{ts}^2\right)+\sum_{t}\sum_{s}\left(b_s^2\right)+2\sum_{s}\left(b_s\left(\sum_{t}a_{ts}\right)\right)$$

식 ()~()에서 본 것 처럼 $\sum_t a_{ts}=0$과 같다. 따라서,

$$\Rightarrow \sum_{t}\sum_{s}\left(a_{ts}^2\right)+\sum_{t}\sum_{s}\left(b_s^2\right)+2\sum_{s}\left(b_s\left(0\right)\right)$$

$$=\sum_{t}\sum_{s}\left(a_{ts}^2\right)+\sum_{t}\sum_{s}\left(b_s^2\right)$$

$a_{ts}$와 $b_{s}$의 정의를 이용해 식을 다시 써주면,

$$\Rightarrow \sum_{t}\sum_{s}\left(X_{ts}-\bar{S}_s\right)^2+\sum_{t}\sum_{s}\left(\bar{S}_s-\bar{X}\right)^2$$

와 같다. 이 식에서 좌측에 있는 값이 $SS_{wit\cdot subj}$이고 우측에 있는 값이 $SS_{bet\cdot subj}$가 된다.

# 구형성

## Mauchly's test

## Epsilon 보정

Greenhouse-Geisser

Huyhn-Feldt

# 실습

MATLAB은 RM ANOVA 사용이 어렵다.

SAS, SPSS는 너무 비싸다.

R을 배울 시간은 또 없다면, Jamovi를 다운받아보자.

## Jamovi

# 참고문헌

* Primer of biostatistics, 7th ed., S. Glantz / Ch. 9 Experiments when each subject receives more than one treatment
* Statistical principles in experimental Design, 1st ed., B. J. Winer / Ch. 4 Single-factor experiments having repeated measures on the same elements